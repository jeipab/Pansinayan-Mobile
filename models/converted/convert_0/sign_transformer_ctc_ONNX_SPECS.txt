
================================================================================
                    ANDROID MODEL SPECIFICATIONS
================================================================================

Generated: 2025-10-23 00:26:16

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
MODEL ARCHITECTURE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Model Type:         Transformer Encoder
Input Dimension:    178 (78 keypoints × 2 coordinates)
Embedding Dim:      256
Attention Heads:    8
Encoder Layers:     4
Pooling Method:     none

Output Classes:
  - Glosses:        10 classes
  - Categories:     1 classes

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
INPUT SPECIFICATIONS (For Android Integration)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Tensor Shape:       [1, T, 178]
  - Batch size:     1 (single sequence)
  - Sequence len:   T (typically 60-120 frames, recommended: 90)
  - Feature dim:    178 (89 keypoints × 2 coordinates)

Data Type:          float32
Value Range:        [0.0, 1.0] (normalized x, y coordinates)

Keypoint Structure:
  [0-49]:    Pose landmarks (25 points × 2) - Upper body
  [50-91]:   Left hand (21 points × 2) - Hand landmarks
  [92-133]:  Right hand (21 points × 2) - Hand landmarks
  [134-177]: Face landmarks (22 points × 2) - Key facial points

Preprocessing:
  1. Extract keypoints with MediaPipe (pose + hands + face)
  2. Normalize coordinates to [0, 1] range
  3. Fill gaps using linear interpolation (max_gap=5)
  4. Arrange in format: [pose, left_hand, right_hand, face]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
OUTPUT SPECIFICATIONS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Output 0 (Gloss Logits):
  - Shape:          [1, 105]
  - Type:           float32
  - Description:    Raw logits for 105 sign gloss classes

Output 1 (Category Logits):
  - Shape:          [1, 10]
  - Type:           float32
  - Description:    Raw logits for 10 category classes

Post-processing:
  1. Apply softmax to convert logits → probabilities
  2. Get argmax for top prediction
  3. Use top-k for confidence ranking
  4. Map ID to label using label_mapping.json

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
FILE SIZES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

sign_transformer_ctc.onnx                  0.28 MB

Recommendation: Use sign_transformer_quant.tflite for Android (smaller & faster)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ANDROID INTEGRATION GUIDE
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. COPY FILES TO ANDROID PROJECT:
   
   Copy to app/src/main/assets/:
   - sign_transformer_quant.tflite
   - label_mapping.json

2. LOAD MODEL IN KOTLIN (TFLiteModelRunner.kt):

   val interpreter = Interpreter(modelBuffer, options)
   
   // Enable GPU acceleration
   if (CompatibilityList().isDelegateSupportedOnThisDevice) {
       options.addDelegate(GpuDelegate())
   }

3. PREPARE INPUT:

   // Keypoint sequence: Array[T, 178] where T = 90 frames
   val inputBuffer = ByteBuffer.allocateDirect(1 * T * 178 * 4)
   inputBuffer.order(ByteOrder.nativeOrder())
   
   for (t in 0 until T) {
       for (i in 0 until 178) {
           inputBuffer.putFloat(sequence[t][i])
       }
   }

4. RUN INFERENCE:

   val glossLogits = Array(1) { FloatArray(105) }
   val categoryLogits = Array(1) { FloatArray(10) }
   
   interpreter.runForMultipleInputsOutputs(
       arrayOf(inputBuffer),
       mapOf(0 to glossLogits, 1 to categoryLogits)
   )

5. POST-PROCESS:

   // Apply softmax
   val probs = softmax(glossLogits[0])
   
   // Get top prediction
   val topIdx = probs.indices.maxByOrNull { probs[it] } ?: 0
   val confidence = probs[topIdx]
   
   // Map to label
   val label = labelMapper.getGlossLabel(topIdx)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PERFORMANCE TIPS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. GPU Acceleration:    2-5× speedup (enable in TFLiteModelRunner)
2. Quantized Model:     4× smaller, 2-3× faster
3. Sequence Length:     Use 60-90 frames (not full 300)
4. Inference Frequency: Run every 10-15 frames, not every frame
5. Async Processing:    Use Kotlin Coroutines (avoid blocking UI)

Expected Performance on Android:
  - Inference Time:     100-300ms per sequence
  - Frame Rate:         10-15 FPS
  - Detection Latency:  500-1000ms (from sign completion)
  - Memory Usage:       100-200MB

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
TROUBLESHOOTING
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Issue: Model file not found in Android
→ Solution: Verify file in app/src/main/assets/sign_transformer_quant.tflite
→ Add to build.gradle: aaptOptions { noCompress "tflite" }

Issue: Inference fails or crashes
→ Solution: Check input tensor shape matches [1, T, 178]
→ Ensure data type is float32
→ Verify sequence length T is reasonable (30-120 frames)

Issue: Slow inference (>500ms)
→ Solution: Enable GPU delegate (see TFLiteModelRunner.kt)
→ Reduce sequence length (90 → 60 frames)
→ Run inference less frequently (every 15 frames instead of 10)

Issue: Poor accuracy on Android
→ Solution: Verify preprocessing matches Python pipeline
→ Check keypoint extraction order (pose, left_hand, right_hand, face)
→ Ensure coordinates normalized to [0, 1] range

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
NEXT STEPS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. Copy model files to Android project (see above)
2. Use provided Kotlin code in tool/kotlin_scaffolding/
3. Follow tool/ANDROID_IMPLEMENTATION_GUIDE.md for complete instructions
4. Test on Android device and verify recognition works

For detailed implementation guide, see:
→ tool/ANDROID_IMPLEMENTATION_GUIDE.md
→ tool/kotlin_scaffolding/PROJECT_STRUCTURE.md

================================================================================
